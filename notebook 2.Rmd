---
title: "R Notebook"
output: html_notebook
---

This markdown is used to replcate the analysis in the blog: https://www.brodrigues.co/blog/2018-11-25-tidy_cv/
https://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/


This blog, comparing to the one in the first notebook showcase the use of tidymodels in hyperparameter tunning.

# Set Up
```{r}
library(tidyverse)
library(tidymodels)
# library(workflows)
# conflicted::conflict_prefer("filter", "dplyr")
ggplot2::theme_set(theme_light())
```


# Data Set: Diamonds
```{r,fig.height=5,fig.width=5}
data("diamonds")
# diamonds %>%
#     sample_n(2000) %>% 
#     mutate_if(is.factor, as.numeric) %>%
#     select(price, everything()) %>%
#     cor %>%
#     {.[order(abs(.[, 1]), decreasing = TRUE), 
#        order(abs(.[, 1]), decreasing = TRUE)]} %>%
#     corrplot::corrplot(method = "number", type = "upper", mar = c(0, 0, 1.5, 0),
#              title = "Correlations between price 
#              and various features of diamonds")
```


# Separating Testing and Training Data: **rsample**

First of all, we want to extract a data set for testing the predictions in the end. We’ll only use a small proportion for training (only to speed things up a little).

```{r}
set.seed(1243)

dia_split <- initial_split(diamonds, prop = .1, strata = price)

dia_train <- training(dia_split)
dia_test  <- testing(dia_split)

dim(dia_train)
dim(dia_test)
```

Furthermore, the training data set will be prepared for 3-fold cross-validation (using three here to speed things up). All this is accomplished using the rsample package
```{r}
dia_vfold <- vfold_cv(dia_train, v = 3, repeats = 1, strata = price)
dia_vfold %>% glimpse()
dia_vfold %>% 
  mutate(df_ana = map(splits, analysis),
         df_ass = map(splits, assessment))
dia_vfold %>% glimpse()
# glance(dia_vfold) Error: C stack usage  15922992 is too close to the limit
```

check how this vfold_cv function is used.
Also, what the analysis and assessment is used??

# Data Pre-Processing and Feature Engineering: recipes
The recipes package can be used to prepare a data set (for modeling) using different step_*() functions. 

For example, the plot below indicates that there may be a nonlinear relationship between price and carat, and I want to address that using higher-order terms.
```{r}
# qplot(carat, price, data = dia_train) +
#     scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2)) +
#     geom_smooth(method = "lm", formula = "y ~ poly(x, 4)") +
#     labs(title = "Nonlinear relationship between price and carat of diamonds",
#          subtitle = "The degree of the polynomial is a potential tuning parameter")
```
The recipe() takes a formula and a data set, and then the different steps are added using the appropriate step_*() functions. The recipes package comes with a ton of useful step functions (see, e.g., vignette("Simple_Example", package = "recipes")).

Herein, I want to log transform price (step_log()), 
I want to center and scale all numeric predictors (step_normalize()), 
and the categorical predictors should be dummy coded (step_dummy()). 
Furthermore, a quadratic effect of carat is added using step_poly().

(do I really need to do dummy variable transformation?? and I dont think a degree 2 polynomial transformation should be called quadratic)
```{r}
dia_rec <-
    recipe(price ~ ., data = dia_train) %>%
    # step_log(all_outcomes()) %>%
    step_log(price) %>%
    step_normalize(all_predictors(), -all_nominal()) %>%
    step_dummy(all_nominal()) %>%
    step_poly(carat, degree = 2) %>% 
    prep() 

dia_rec
```

Calling prep() on a recipe applies all the steps. You can now call juice() to extract the transformed data set or call bake() on a new data set.
```{r}
dia_juiced <- juice(dia_rec)
dim(dia_juiced)
```

# Defining and Fitting Models: parsnip

The models are separated into two modes/categories, namely, regression and classification (set_mode()). The model is defined using a function specific to each algorithm (e.g., linear_reg(), rand_forest()). Finally, the backend/engine/implementation is selected using set_engine()


```{r}
lm_model <-
    linear_reg() %>%
    set_mode("regression") %>%
    set_engine("lm")

# rand_forest(mtry = 3, trees = 500, min_n = 5) %>%
#     set_mode("regression") %>%
#     set_engine("ranger", importance = "impurity_corrected")

lm_fit1 <- lm_model %>% 
  fit(price ~ ., dia_juiced)
lm_fit1
```

Unnecessary variables can easily be dropped in the recipe using step_rm()


# Summarizing Fitted Models: broom: glance tidy and augment
Many models have implemented summary() or coef() methods. However, the output of these is usually not in a tidy format, and the broom package has the aim to resolve this issue.

```{r}
glance(lm_fit1$fit)
tidy(lm_fit1)
lm_predicted <- 
  augment(lm_fit1$fit, data = dia_juiced) %>% 
  rowid_to_column()
lm_predicted %>% 
  select( rowid, price, .fitted:.std.resid)
```

A plot of the predicted vs. actual prices shows small residuals with a few outliers, which are not well explained by the model.
```{r}
ggplot(lm_predicted, aes(.fitted, price)) +
    geom_point(alpha = .2) +
    ggrepel::geom_label_repel(aes(label = rowid), 
                              data = filter(lm_predicted, abs(.resid) > 2)) +
    labs(title = "Actual vs. Predicted Price of Diamonds"
```

# Evaluating Model Performance: yardstick::bake()
together with rsample::analysis, assessment and bake

We already saw performance measures RMSE and R squared in the output of glance() above. The yardstick package is specifically designed for such measures for both numeric and categorical outcomes, and it plays well with multiple predictions.

Let’s use rsample, parsnip, and yardstick for cross-validation to get a more accurate estimation of RMSE.

In the following pipeline, the model is fit() separately to the three analysis data sets, and then the fitted models are used to predict() on the three corresponding assessment data sets (i.e., 3-fold cross-validation). 

Before that, analysis() and assessment() are used to extract the respective folds, and bake() is used to apply the recipe steps to these data sets. 
(Note that one could have created dia_vfold using the prepared rather than the raw data in the first place to get rid of these two bake() calls.)

Herein, I use list columns to store all information about the three folds in one data frame and 

a combination of dplyr::mutate() and purrr::map() to “loop” across the rows of the data frame.
```{r}
lm_fit2 <- dia_vfold %>% 
    mutate(
        df_ana = map(splits,  analysis),
        df_ana = map(df_ana, ~bake(dia_rec, .x)),
        df_ass = map(splits,  assessment),
        df_ass = map(df_ass, ~bake(dia_rec, .x)),
        model_fit  = map(df_ana, ~fit(lm_model, price ~ ., data = .x)),
        model_pred = map2(model_fit, df_ass, ~predict(.x, new_data = .y)))

```

Now, we can extract the actual prices from the assessment data and compare them to the predicted prices. Across the three folds, we see that the RMSE is a little higher and R squared a little smaller compared to above. This is expected, since out-of-sample prediction is harder but also way more useful.
```{r}
lm_fit2 %>% 
    mutate(res = map2(df_ass, model_pred, ~data.frame(price = .x$price,
                                                      .pred = .y$.pred))) %>% 
    select(id, res) %>% 
    unnest(res) %>% 
    group_by(id) %>% 
    metrics(truth = price, estimate = .pred)

```

Note that yardstick::metrics() has default measures for numeric and categorical outcomes, and here RMSE, R squared, and the mean absolute difference (MAE) is returned. You could also use one metric directly like rmse or define a custom set of metrics via metric_set().


# Tuning Model Parameters: tune and dials

Let’s get a little bit more involved and do some hyperparameter tuning. We turn to a different model, namely, a random forest model.

### overall
The tune package has functions for doing the actual tuning (e.g., via grid search), while all the parameters and their defaults (e.g., mtry(), neighbors()) are implemented in dials. Thus, the two packages can almost only be used in combination.


### Preparing a parsnip Model for Tuning
First, I want to tune the mtry parameter of a random forest model. Thus, the model is defined using parsnip as above. 
However, rather than using a default value (i.e., mtry = NULL) or one specific value (i.e., mtry = 3), we use tune() as a placeholder and let cross-validation decide on the best value for mtry later on.

As the output indicates, the default minimum of mtry is 1 and the maximum depends on the data.

```{r}
rf_model <- 
    rand_forest(mtry = tune()) %>%
    set_mode("regression") %>%
    set_engine("ranger")

parameters(rf_model)
mtry()
```


Thus, this model is not yet ready for fitting. You can either specify the maximum for mtry yourself using update(), or you can use finalize() to let the data decide on the maximum.

(Please check here what is happening.. what does it mean for maximum of mtry here??)
```{r}
rf_model %>% 
    parameters %>% 
    update(mtry = mtry(c(1L, 5L)))

rf_model %>% 
    parameters %>% 
    # Here, the maximum of mtry equals the number of predictors, i.e., 24.
    finalize(x = select(juice(dia_rec), -price)) %>% 
    magrittr::extract2("object")
```


### Preparing Data for Tuning: recipes
The second thing I want to tune is the degree of the polynomial for the variable carat. As you saw in the plot above, polynomials up to a degree of four seemed well suited for the data. However, a simpler model might do equally well, and we want to use cross-validation to decide on the degree that works best.

** Similar to tuning parameters in a model, certain aspects of a recipe can be tuned.** 
Let’s define a second recipe and use tune() inside step_poly().
```{r}
dia_rec2 <-
    recipe(price ~ ., data = dia_train) %>%
    step_log(all_outcomes()) %>%
    step_normalize(all_predictors(), -all_nominal()) %>%
    step_dummy(all_nominal()) %>%
    step_poly(carat, degree = tune())

dia_rec2 %>% 
    parameters() %>% 
    magrittr::extract2("object")
dia_rec2
```
why the range of polynomial degree is [1:3] here? Is this because we have only three CV split and the options for the degree starts with 1?

### Combine Everything: workflows
The workflows package is designed to bundle together different parts of a machine learning pipeline like a recipe or a model.

First, let’s create an initial workflow and add the recipe and the random forest model, both of which have a tuning parameter.

```{r}
rf_wflow <-
    workflow() %>%
    add_model(rf_model) %>%
    add_recipe(dia_rec2)
rf_wflow
```

Second, we need to update the parameters in rf_wflow, because the maximum of mtry is not yet known and the maximum of degree should be four (while three is the default).

```{r}
rf_param <-
    rf_wflow %>%
    parameters() %>%
    update(mtry = mtry(range = c(3L, 5L)),
           degree = degree_int(range = c(2L, 4L)))
rf_param$object
```

Third, we want to use cross-validation for tuning, that is, to select the best combination of the hyperparameters. 
Bayesian optimization (see vignette("svm_classification", package = "tune")) is recommended for complex tuning problems, and this can be done using tune_bayes().

Herein, however, grid search will suffice. To this end, let’s create a grid of all necessary parameter combinations.
```{r}
rf_grid <- grid_regular(rf_param, levels = 3)
rf_grid
```

Cross-validation and hyperparameter tuning can involve fitting many models. Herein, for example, we have to fit 3 x 9 models (folds x parameter combinations). 

To increase speed, we can fit the models in parallel. This is directly supported by the tune package (see vignette("optimizations", package = "tune")).

```{r}
library("doFuture")
all_cores <- parallel::detectCores(logical = T)

registerDoFuture()
cl <- makeCluster(all_cores-1)
plan(future::cluster, workers = cl)
```

Then, we can finally start tuning. The results can be examined using autoplot() and show_best():

I am still not sure about how grid search works with CV, did they form a hierachical search? (on each split, combinations of different parameters is used to fit the model )
```{r}
rf_search <- tune_grid(rf_wflow, 
                       grid = rf_grid, 
                       resamples = dia_vfold,
                       param_info = rf_param)

autoplot(rf_search, metric = "rmse") +
    labs(title = "Results of Grid Search for Two Tuning Parameters of a Random Forest")
```

